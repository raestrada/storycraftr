# RAG Evaluation Frameworks

This document provides an overview of open-source frameworks for evaluating Retrieval-Augmented Generation (RAG) systems. It informed our decision to adopt RAGAS for quality assurance in StoryCraftr.

## Framework Selection: RAGAS

For StoryCraftr, we have selected **RAGAS (RAG Assessment Suite)** as our primary evaluation framework. The key reasons for this choice are:

-   **Wide Adoption and Maturity:** RAGAS is the most widely adopted open-source framework, benefiting from a large community and comprehensive documentation.
-   **Reference-Free Evaluation:** It uses LLMs as judges, eliminating the need for manually curated "golden" datasets, which is ideal for the dynamic and creative content generated by StoryCraftr.
-   **Comprehensive Metrics:** RAGAS provides a suite of metrics crucial for prose quality, including:
    -   **Faithfulness:** Measures if the generated answer is true to the retrieved context.
    -   **Answer Relevancy:** Assesses if the answer is relevant to the query.
    -   **Context Precision & Recall:** Evaluates the quality and completeness of the retrieved context.

While other frameworks offer specialized features, RAGAS provides the best balance of flexibility, comprehensiveness, and ease of integration for our initial needs.

---

## Survey of RAG Evaluation Frameworks

The following is a summary of prominent open-source RAG evaluation frameworks that were considered.

### Comprehensive Frameworks

**RAGAS (RAG Assessment Suite)**
- **Description:** The most widely adopted framework. Provides reference-free evaluation using LLMs as judges.
- **Key Metrics:** Faithfulness, Context Relevance, Answer Relevancy, Context Recall, Context Precision.

**Open RAG Eval**
- **Description:** A framework by Vectara that does not require predefined "golden answers."
- **Key Metrics:** UMBRELA (retrieval quality), AutoNugget (information capture), and Citation verification.

### Specialized Benchmarks

**RGB (Retrieval-Augmented Generation Benchmark)**
- **Description:** Specifically tests four fundamental RAG abilities across different context scenarios.
- **Focus Areas:** Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness.

**MIRAGE**
- **Description:** Designed to evaluate RAG adaptability across different context window sizes.
- **Focus Areas:** Context acceptability, noise vulnerability, and context utilization.

### Implementation-Ready Tools

**LlamaIndex**
- **Description:** Provides built-in evaluation modules for response and retrieval quality assessment.
- **Key Metrics:** Faithfulness, Relevancy, Correctness.

**LangChain**
- **Description:** Includes RAG evaluation capabilities through integration with frameworks like RAGAS.
- **Key Metrics:** Contextual Precision, Recall, Relevancy.
