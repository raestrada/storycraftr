# RAG Evaluation Frameworks

This document provides an overview of open-source frameworks for evaluating Retrieval-Augmented Generation (RAG) systems. It informed our decision to adopt RAGAS for quality assurance in StoryCraftr.

## Framework Selection: RAGAS

For StoryCraftr, we have selected **RAGAS (RAG Assessment Suite)** as our primary evaluation framework. The key reasons for this choice are:

- **Wide Adoption and Maturity:** RAGAS is the most widely adopted open-source framework, benefiting from a large community and comprehensive documentation.
- **Reference-Free Evaluation:** It uses LLMs as judges, eliminating the need for manually curated "golden" datasets, which is ideal for the dynamic and creative content generated by StoryCraftr.
- **Comprehensive Metrics:** RAGAS provides a suite of metrics crucial for prose quality, including:
  - **Faithfulness:** Measures if the generated answer is true to the retrieved context.
  - **Answer Relevancy:** Assesses if the answer is relevant to the query.
  - **Context Precision & Recall:** Evaluates the quality and completeness of the retrieved context.

While other frameworks offer specialized features, RAGAS provides the best balance of flexibility, comprehensiveness, and ease of integration for our initial needs.

---

## Survey of RAG Evaluation Frameworks

The following is a summary of prominent open-source RAG evaluation frameworks that were considered.

### Comprehensive Frameworks

**RAGAS (RAG Assessment Suite)**

- **Description:** The most widely adopted framework. Provides reference-free evaluation using LLMs as judges.
- **Key Metrics:** Faithfulness, Context Relevance, Answer Relevancy, Context Recall, Context Precision.

**Open RAG Eval**

- **Description:** A framework by Vectara that does not require predefined "golden answers."
- **Key Metrics:** UMBRELA (retrieval quality), AutoNugget (information capture), and Citation verification.

### Specialized Benchmarks

**RGB (Retrieval-Augmented Generation Benchmark)**

- **Description:** Specifically tests four fundamental RAG abilities across different context scenarios.
- **Focus Areas:** Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness.

**MIRAGE**

- **Description:** Designed to evaluate RAG adaptability across different context window sizes.
- **Focus Areas:** Context acceptability, noise vulnerability, and context utilization.

### Implementation-Ready Tools

**LlamaIndex**

- **Description:** Provides built-in evaluation modules for response and retrieval quality assessment.
- **Key Metrics:** Faithfulness, Relevancy, Correctness.

**LangChain**

- **Description:** Includes RAG evaluation capabilities through integration with frameworks like RAGAS.
- **Key Metrics:** Contextual Precision, Recall, Relevancy.

---

## Implementation Example with `pytest`

To integrate RAGAS into our test suite, we can create evaluation tests that generate answers using our RAG pipeline and then measure their quality against our metrics. These tests require a live LLM endpoint and should be marked accordingly.

Here is a conceptual example of a RAG evaluation test:

```python
import os
import pytest
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

# Assume 'rag_pipeline' is our function that takes a question
# and returns an answer and the retrieved contexts.
from storycraftr.rag.pipeline import rag_pipeline

@pytest.mark.skipif(
    not os.environ.get("OPENAI_API_KEY"),
    reason="OPENAI_API_KEY environment variable not set",
)
def test_rag_faithfulness_and_relevancy():
    # 1. Prepare a test dataset
    data_samples = {
        'question': ["What is the primary theme of the protagonist's journey?"],
        'ground_truth': ["The primary theme is the search for identity."]
    }
    dataset = Dataset.from_dict(data_samples)

    # 2. Run the RAG pipeline to get answers and contexts
    results = [rag_pipeline(q) for q in dataset["question"]]
    answers = [r['answer'] for r in results]
    contexts = [r['contexts'] for r in results]

    # 3. Create a new dataset with generated results for evaluation
    result_dataset = Dataset.from_dict({
        'question': dataset['question'],
        'answer': answers,
        'contexts': contexts,
    })

    # 4. Evaluate using RAGAS
    score = evaluate(
        result_dataset,
        metrics=[faithfulness, answer_relevancy],
    )

    # 5. Assert that the scores meet a minimum threshold
    assert score['faithfulness'] > 0.8
    assert score['answer_relevancy'] > 0.8
```

This example demonstrates how to use `datasets` and `ragas` to create an evaluative test that can be run as part of our CI/CD pipeline, ensuring that changes to the RAG system do not degrade response quality.
