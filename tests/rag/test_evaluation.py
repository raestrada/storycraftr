import os
import pytest
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

# This is a synthetic dataset for demonstration purposes.
# 'question': The user query.
# 'answer': The answer generated by the RAG system.
# 'contexts': The context chunks retrieved from the vector store.
# 'ground_truth': The reference answer for evaluation.
data_samples = {
    'question': [
        "What is the capital of France?",
        "Who wrote 'To Kill a Mockingbird'?",
    ],
    'answer': [
        "Paris is the capital of France.",
        "Harper Lee wrote 'To Kill a Mockingbird'.",
    ],
    'contexts': [
        ["France is a country in Western Europe. Its capital is Paris, a major European city."],
        ["'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize."],
    ],
    'ground_truth': [
        "The capital of France is Paris.",
        "Harper Lee is the author of 'To Kill a Mockingbird'.",
    ]
}
dataset = Dataset.from_dict(data_samples)

@pytest.mark.skipif(
    not os.environ.get("OPENAI_API_KEY"),
    reason="OPENAI_API_KEY environment variable not set",
)
def test_rag_evaluation_metrics():
    """
    Tests the RAG evaluation metrics using a synthetic dataset.
    This test requires an OpenAI API key to run, as RAGAS uses an LLM for evaluation.
    """
    metrics = [
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision,
    ]

    result = evaluate(
        dataset=dataset,
        metrics=metrics,
    )

    assert result is not None
    # For this ideal synthetic data, we expect scores to be high.
    # Using a threshold instead of 1.0 to avoid flakiness from LLM evaluation.
    assert result["faithfulness"] > 0.9
    assert result["answer_relevancy"] > 0.9
    assert result["context_recall"] > 0.9
    assert result["context_precision"] > 0.9
